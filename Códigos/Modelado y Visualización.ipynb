{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP y Visualización\n",
    "\n",
    "Input: Datasets Predios, Publicaciones Científicas, Nombre Especies, Países\n",
    "Output: Análisis de tendencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importar librerias\n",
    "\n",
    "import json\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "#from spacy import displacy\n",
    "\n",
    "import pandas as pd\n",
    "import plotly_express as px\n",
    "#from spacy.tokens import DocBin\n",
    "from collections import Counter\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "\n",
    "import dash\n",
    "from dash import dcc, html, Input, Output, dash_table,callback, Dash\n",
    "\n",
    "from dash.exceptions import PreventUpdate\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "import dash_bootstrap_components as dbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar datasets: \n",
    "Predios, Publicaciones Científicas, Nombre Especies y Países"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:9: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:12: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:9: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:12: SyntaxWarning: invalid escape sequence '\\A'\n",
      "C:\\Users\\56985\\AppData\\Local\\Temp\\ipykernel_11024\\3615361628.py:2: SyntaxWarning: invalid escape sequence '\\A'\n",
      "  df_predios=pd.read_excel('..\\Archivos\\Terrenos.xlsx',sheet_name='Sheet1')\n",
      "C:\\Users\\56985\\AppData\\Local\\Temp\\ipykernel_11024\\3615361628.py:5: SyntaxWarning: invalid escape sequence '\\A'\n",
      "  with open ('..\\Archivos\\pubmed_data262.json','r',encoding='utf-8') as f:\n",
      "C:\\Users\\56985\\AppData\\Local\\Temp\\ipykernel_11024\\3615361628.py:9: SyntaxWarning: invalid escape sequence '\\A'\n",
      "  df_especies=pd.read_excel('..\\Archivos\\Terrenos.xlsx',sheet_name='Hoja1')\n",
      "C:\\Users\\56985\\AppData\\Local\\Temp\\ipykernel_11024\\3615361628.py:12: SyntaxWarning: invalid escape sequence '\\A'\n",
      "  codigo_paises=pd.read_excel('..\\Archivos\\Copia de código paises.xlsx')\n"
     ]
    }
   ],
   "source": [
    "# Dataset predios\n",
    "df_predios=pd.read_excel('..\\Archivos\\Terrenos.xlsx',sheet_name='Sheet1')\n",
    "\n",
    "#Dataset Publicaciones Científicas\n",
    "with open ('..\\Archivos\\pubmed_data262.json','r',encoding='utf-8') as f:\n",
    "    pubmed_data=json.load(f)\n",
    "\n",
    "#Dataset Nombre Especies\n",
    "df_especies=pd.read_excel('..\\Archivos\\Terrenos.xlsx',sheet_name='Hoja1')\n",
    "\n",
    "#Dataset Países\n",
    "codigo_paises=pd.read_excel('..\\Archivos\\Copia de código paises.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar GeoJSON\n",
    "with open('..\\\\Archivos\\\\terrenos_enriquecido_ID.geojson', \"r\", encoding=\"utf-8\") as f:\n",
    "    geojson_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_predios=pd.merge(df_predios,df_especies[['ID_ESP1','ID','Nombre científico']])\n",
    "df_predios_group=df_predios.groupby(['USO_CEFOR','Nombre científico','ID'])['SUP_HA'].sum().reset_index()\n",
    "df_predios_group_lplp=df_predios.groupby(['NOMBRE','Nombre científico','ID'])['SUP_HA'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\A'\n",
      "C:\\Users\\56985\\AppData\\Local\\Temp\\ipykernel_11024\\3546564186.py:1: SyntaxWarning: invalid escape sequence '\\A'\n",
      "  ruta_imagen=Image.open('..\\Archivos\\Logo_udec_maci.png')\n"
     ]
    }
   ],
   "source": [
    "ruta_imagen=Image.open('..\\Archivos\\Logo_udec_maci.png')\n",
    "#ruta_imagen_predios=Image.open('Terrenos.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordcloud\n",
    "\n",
    "def procesar_keywords (keywords):\n",
    "\n",
    "    kw=[]\n",
    "    for keyword in keywords:\n",
    "        for key in keyword:\n",
    "            kw.append(key)\n",
    "\n",
    "    keywords_string=' ,'.join(e for e in kw)\n",
    "    return keywords_string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NPL y Visualización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\A'\n",
      "C:\\Users\\56985\\AppData\\Local\\Temp\\ipykernel_11024\\3952774604.py:1: SyntaxWarning: invalid escape sequence '\\A'\n",
      "  with open('..\\Archivos\\limites_unificados.geojson', \"r\", encoding=\"utf-8\") as f:\n"
     ]
    }
   ],
   "source": [
    "with open('..\\Archivos\\limites_unificados.geojson', \"r\", encoding=\"utf-8\") as f:\n",
    "    geojson_limite = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8053/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1aab879c320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#probando esto\n",
    "\n",
    "app=dash.Dash()\n",
    "\n",
    "#funciones globales:\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")     #cargo el modelo\n",
    "\n",
    "options_dict=dict(zip(df_especies['Nombre científico'],df_especies['ID']))     #defino las opciones del menú desplegable\n",
    "\n",
    "def clean_text(texto):                                                         #defino la función de limpieza del texto\n",
    "    return re.sub(r\"<.*?>|[%()=/-]|\\d\", \"\", str(texto)) #nueva versión\n",
    "    #return re.sub(\"<.*?>|%|\\(|\\)|=|-/n|/>-|\\d\", \"\", str(texto))\n",
    "\n",
    "def procesar_json_fil(diccionario):                                         #esta es la función de procesamiento principal\n",
    "    lista=[]\n",
    "    data={}\n",
    "\n",
    "    for entry in diccionario:#lista-dicc. accedo al primer elemento\n",
    "        \n",
    "        for doc_id , text in entry.items():\n",
    "            doc = nlp(clean_text(text))                                      #llamo a la función de limpieza\n",
    "            data_id=[]\n",
    "            for token in doc:\n",
    "                if token.pos_ in [\"NOUN\",\"PROPN\"] and len(token.lemma_)>3 and not token.is_stop:\n",
    "                    \n",
    "                    data_id.append(token.lemma_.lower())\n",
    "                data[doc_id]=data_id\n",
    "    lista.append(data)                                                      #parece que esta parte está un poco demás\n",
    "  \n",
    "    #lista de todas las claves\n",
    "    M=[]\n",
    "    for x,y in data.items():\n",
    "        for e in y:\n",
    "            M.append(e)\n",
    "    L=list(set(M))\n",
    "    #procesamiento de frecuencias\n",
    "    filas=[]\n",
    "    pal_tot=len(M)\n",
    "    lis_tot=len(diccionario)\n",
    "\n",
    "    for e in L:\n",
    "        total=0\n",
    "        listas_presentes=[]\n",
    "\n",
    "        for x,y in data.items():\n",
    "            count = y.count(e)\n",
    "            total += count\n",
    "            if count>0:\n",
    "                listas_presentes.append(x)\n",
    "\n",
    "        filas.append([e,listas_presentes,total/pal_tot*100,len(listas_presentes),len(listas_presentes)/lis_tot*100,total])\n",
    "        \n",
    "    df_res_res=pd.DataFrame(filas,columns=[\"lemma\",\"listas presentes\",\"f abs\",\"cantidad listas\",\"f rel\",\"total\"])\n",
    "    return df_res_res\n",
    "\n",
    "#Procesar json: obtener países\n",
    "\n",
    "#probar generando una lista de dicc\n",
    "\n",
    "def procesar_paises (valor):\n",
    "\n",
    "    #valor=[31,18,25]\n",
    "    lista_pmids=[]\n",
    "\n",
    "    for paper in pubmed_data:\n",
    "        if paper['ID'] in valor:\n",
    "            lista_pmids.append(paper['PMID'])\n",
    "\n",
    "    #aff=[]\n",
    "    #pm=[]\n",
    "    A={}\n",
    "    D=[]\n",
    "    #Lista=[]\n",
    "    for record in pubmed_data:\n",
    "        if record['PMID'] in lista_pmids:\n",
    "                    \n",
    "            for autor in record['Authors']:\n",
    "                affi=autor['Affiliations']\n",
    "                for inst in affi:\n",
    "                \n",
    "                    pm2=\"\".join(record['PMID'])\n",
    "                    aff2=\"\".join(inst)\n",
    "                    A[pm2]=aff2\n",
    "                    D.append(A)\n",
    "\n",
    "\n",
    "    #lista=[]\n",
    "    data=[]\n",
    "\n",
    "    for entry in D:\n",
    "        for doc_id, text in entry.items():\n",
    "\n",
    "            doc=nlp(clean_text(text))\n",
    "            paises=[ent.text for ent in doc.ents if ent.label_ ==\"GPE\"]\n",
    "            for pais in set(paises):\n",
    "                    data.append({\"PMID\":doc_id,\"GPE\":pais})\n",
    "\n",
    "    df_p=pd.DataFrame(data)\n",
    "\n",
    "    #df_p[\"PMID\"] = pd.to_numeric(df_p[\"PMID\"],errors=\"coerce\")\n",
    "    df_p_unique=df_p.drop_duplicates(subset=[\"PMID\",\"GPE\"])\n",
    "\n",
    "    df_unique_conteo=df_p_unique['GPE'].value_counts().reset_index()\n",
    "    df_unique_conteo.columns=['País','Recuento']\n",
    "    df_unique_conteo_codigo=pd.merge(df_unique_conteo,codigo_paises,left_on=\"País\",right_on=\"Country\")\n",
    "\n",
    "    return df_unique_conteo_codigo\n",
    "\n",
    "\n",
    "\n",
    "#Inicio app\n",
    "\n",
    "app=Dash(__name__,external_stylesheets=[dbc.themes.LUX,dbc.icons.FONT_AWESOME])\n",
    "\n",
    "app.layout=dbc.Container([\n",
    "    html.Br(),\n",
    "    dbc.Row([dbc.Col([html.Img(src=ruta_imagen,style={'height':'40px','width':'10%'})])]),\n",
    "    html.Br(),\n",
    "    html.H4(\"Tesina Beatriz Ugalde - 2025\"),\n",
    "    html.Br(),\n",
    "    html.Br(),\n",
    "    html.H1(\"Identificar líneas de trabajo para Potenciar Investigación y Docencia en predios forestales\",style={'text-align':'center','margin-top':'20px'}),\n",
    "    html.Br(),\n",
    "    html.Br(),\n",
    "    html.Br(),\n",
    "    html.P(\"Se busca potenciar la investigación y docencia en predios forestales. \"\n",
    "\n",
    "\"Se muestra la caracterización de dos predios en términos de composición y  uso de suelo. \" \n",
    "\n",
    "\"Se realiza una búsqueda de publicaciones científicas relacionadas con las especies que los habitan, se extraen términos relevantes y se analiza su importancia en función de su presencia y abundancia, promoviendo la búsqueda de tendencias innovadoras en investigación científica.\",style={'text-align':'justify'}),\n",
    "    html.Br(),\n",
    "    html.Br(),\n",
    "    html.H4(\"Caracterización predios forestales\"),\n",
    "    html.Br(),\n",
    "    html.P(\"El centro forestal (CEFOR) perteneciente a la Universidad Austral de Chile, cuenta con un patrimonio de más de 4000 hectáreas de predios forestales, donde se realizan labores de docencia, investigación y extensión. Dos de los predios más antiguos, “Los Pinos“ y “Las Palmas“ cuentan con cerca de 1500 hectáreas de terreno, \"\n",
    "    \"están ubicados a 5 Km del Campus Isla Teja, y presentan gran valor biológico. \"  \n",
    "    \" Seleccione una o más especies para conocer su ubicación, uso y potenciales líneas de investigación\", style={'text-align':'justify'}),\n",
    "    #html.H6(\"Seleccione una o más especies\"),\n",
    "    html.Br(),\n",
    "    html.Div([ dcc.Dropdown(options=[{'label':NOM,'value':ID} for NOM,ID in sorted(options_dict.items(),key=lambda x: x[1], reverse=True)],id=\"entrada\",multi=True)]),\n",
    "    #html.Div([dcc.Input(),dcc.Input(style={'margin_left':'15px'})]),\n",
    "    html.Br(),\n",
    "    html.H6(\"Descripción de las especies seleccionadas\"),\n",
    "    html.Br(),\n",
    "    dbc.Row([dbc.Col([dash_table.DataTable(id='tabla_nombres',style_cell={'textAlign':'left'})]),dbc.Col([dcc.Graph(id='mapa-terrenos', style={\"height\": \"80vh\"})])]),\n",
    "    html.Br(),\n",
    "    html.Br(),\n",
    "    #html.Div([dcc.Graph(id='mapa-terrenos', style={\"height\": \"80vh\"})]),\n",
    "    #html.H6(\"Relación de las especies con los predios\"),\n",
    "    dbc.Row([\n",
    "        dbc.Col([dcc.Graph(id='predios_barras',style={'height':'400px','width':'100%'})]),dbc.Col([dcc.Graph(id='predios_barras_lplp',style={'height':'400px','width':'100%'})])]),\n",
    "    html.Br(),\n",
    "    html.Br(),\n",
    "    html.H4(\"Extracción de términos desde publicaciones científicas\"),\n",
    "    html.Br(),\n",
    "    html.P(\"A partir de las especies seleccionadas en la parte superior, Se obtienen publicaciones científicas desde pubmed central si la especie aparece en el título o abstract.\" \n",
    "\"Se muestra una nube de palabras que agrupa todas las “keywords“ de las publicaciones seleccionadas,  la evolución de las publicaciones en el tiempo y los países donde se investigan dichas especies.\"),\n",
    "    html.Br(),\n",
    "            \n",
    "    dbc.Row([\n",
    "        dbc.Col([html.Img(id='wordcloud',style={'height':'100%','width':'95%'})]),\n",
    "        dbc.Col([dcc.Graph(id='paper_año',style={'height':'100%','width':'95%'})]),\n",
    "        dbc.Col([dcc.Graph(id='mapa_paises',style={'height':'100%','width':'95%'})])],style={'height':'400px'}),\n",
    "    html.Br(),\n",
    "    html.Br(),    \n",
    "    html.P(\"A continuación se realiza un análisis semántico de los abstracts y se extraen términos importantes  y se analizan en función de su presencia y abundancia. La presencia se determina evaluando en cuántas publicaciones diferentes aparece cada término, y la abundancia cuántas veces aparece en total. Así, un término altamente presente y abundante se espera que sea de poca importancia, ya que es transversal a todas las publicaciones, ahora bien un término poco presente podría representar una línea de trabajo innovadora.\"),\n",
    "\n",
    "    dcc.Graph(id='grafico'),\n",
    "    html.Br(),\n",
    "    html.H6(\"Información relacionada al término seleccionado:\"),\n",
    "    html.Br(),\n",
    "    dbc.Row([dbc.Col([dash_table.DataTable(id='tabla',style_cell={'textAlign':'left'},style_data={'whiteSpace':'normal','width':'100px','maxWidth':'100px','minWidth':'100px','height':'auto'},style_cell_conditional=[{'if':{'column_id':'Título'},'width':'20%'},{'if':{'column_id':'Institución'},'width':'20%'}])])]),\n",
    "    html.Br(),\n",
    "    html.P(\"Para más información, visite la URL\")\n",
    "    ])\n",
    "        \n",
    "\n",
    "#aquí debería ir el callback del tipo de especies: va a limitar las opciones que aparezcan a continuación\n",
    "\n",
    "@callback(\n",
    "        Output(\"predios_barras\",\"figure\"),\n",
    "        Output(\"predios_barras_lplp\",\"figure\"),\n",
    "        Output(\"tabla_nombres\",\"data\"),\n",
    "        Output(\"tabla_nombres\",\"columns\"),\n",
    "        Output('wordcloud','src'),\n",
    "        Output('paper_año','figure'),\n",
    "        Output('mapa_paises','figure'),\n",
    "        Output(\"grafico\",\"figure\"),\n",
    "        Output(\"mapa-terrenos\", \"figure\"),\n",
    "        Input(\"entrada\",\"value\"))\n",
    "\n",
    "#Primero filtrar el JSON por especie\n",
    "\n",
    "def filtro(valor):\n",
    "\n",
    "    if valor is None:  # Si no hay selección\n",
    "        raise PreventUpdate\n",
    "    else:\n",
    "        #Filtrar por nombre/ID\n",
    "        df_especies_filtrado = df_especies[df_especies['ID'].isin(valor)]\n",
    "        nombre_cien= df_especies_filtrado['Nombre científico']   \n",
    "        nombre_com= df_especies_filtrado['Nombre común']\n",
    "        nombre_ori=df_especies_filtrado['Nombre originario']         \n",
    "        df=pd.DataFrame({'Nombre científico':nombre_cien,'Nombre común':nombre_com,'Nombre originario':nombre_ori})\n",
    "        data=df.to_dict('records')\n",
    "        columns=[{'id':c,'name':c}for c in df.columns]\n",
    "\n",
    "        #esta parte filtra el json: abstract\n",
    "        json_filtrado = [{paper['PMID']: paper['Abstract']} for paper in pubmed_data if paper['ID'] in valor]\n",
    "        fig=px.scatter(procesar_json_fil(json_filtrado),x='f rel',y='f abs',size='total',hover_data=['listas presentes'],hover_name='lemma',opacity=0.5,title=\"Presencia y Abundancia de términos\",labels={'f rel':'Presencia','f abs':'Abundancia'})\n",
    "\n",
    "        #esta parte filtra el json: solo pmids:lista\n",
    "        lista_pmids=[]\n",
    "        for paper in pubmed_data:\n",
    "            if paper['ID'] in valor:\n",
    "                lista_pmids.append(paper['PMID'])\n",
    "\n",
    "        #Supongo que aquí tiene que ir para el mapa mundi\n",
    "        # \n",
    "        \n",
    "        \n",
    "        #llamar a función paises y graficar mapa mundi\n",
    "        \n",
    "        #keywords y año\n",
    "        keywords=[]\n",
    "        año_paper=[]\n",
    "        pubmedid=[]\n",
    "        nombrecien=[]\n",
    "\n",
    "        for paper in pubmed_data:\n",
    "            if paper['ID'] in valor:\n",
    "                keywords.append(paper['Keywords'])\n",
    "                año_paper.append(paper['Date']['Year'])\n",
    "                pubmedid.append(paper['PMID'])\n",
    "                nombrecien.append(paper['Topic'])\n",
    "        wc=WordCloud(background_color='white',colormap='Greens',width=400,height=400).generate(procesar_keywords(keywords))\n",
    "        wc.to_file(\"wordcloudkw.png\")\n",
    "        encoded_image= Image.open('wordcloudkw.png')\n",
    "        \n",
    "        #gráfico papers por año\n",
    "        df_paper_año=pd.DataFrame({'PMID':pubmedid,\"Año\":año_paper,\"Nombre científico\":nombrecien})\n",
    "        df_paper_año=df_paper_año.groupby(['Nombre científico','Año'])['PMID'].count().reset_index()\n",
    "        fig_pap_año=px.bar(df_paper_año,x=\"Año\",y=\"PMID\",color='Nombre científico',labels={'PMID':'Número de Publicaciones'},title='Número de papers publicados por año',width=400,height=400)\n",
    "        fig_pap_año.update_layout(xaxis={'categoryorder':'category ascending'},margin={\"r\":10,\"t\":10,\"l\":10,\"b\":10})\n",
    "\n",
    "        fig_mapa=px.choropleth(procesar_paises(valor),locations=\"Alpha-3 code\",color=\"Recuento\",color_continuous_scale='Greens',projection='natural earth',title='Países donde se investiga',width=400,height=400)\n",
    "        fig_mapa.update_geos(showcoastlines=False,framewidth=0.3,framecolor=\"#444\")\n",
    "        fig_mapa.update_layout(coloraxis_showscale=True)\n",
    "        #fig_mapa.update_layout()\n",
    "\n",
    "\n",
    "        #filtro el dataset predios\n",
    "        df_predios_group_filtro = df_predios_group[df_predios_group['ID'].isin(valor)]\n",
    "        fig_predios=px.bar(df_predios_group_filtro,x='USO_CEFOR',y='SUP_HA',color='Nombre científico',text_auto=True,labels={'SUP_HA':'Superficie (Há)','USO_CEFOR':'Uso de suelo'},title='Superficie por uso de suelo y especie')\n",
    "        fig_predios.update_layout(xaxis={'categoryorder':'total descending'})\n",
    "\n",
    "        df_predios_group_filtro_lplp = df_predios_group_lplp[df_predios_group_lplp['ID'].isin(valor)]\n",
    "        fig_predios_lplp=px.bar(df_predios_group_filtro_lplp,x='NOMBRE',y='SUP_HA',color='Nombre científico',text_auto=True,labels={'SUP_HA':'Superficie (Há)','NOMBRE':'Predio'},title='Superficie por predio y especie')\n",
    "        fig_predios_lplp.update_layout(xaxis={'categoryorder':'total descending'})\n",
    "\n",
    "    #mapa coropletico\n",
    "\n",
    "    if not valor:\n",
    "        features_filtrados = geojson_data[\"features\"]\n",
    "    else:\n",
    "        features_filtrados = [\n",
    "            f for f in geojson_data[\"features\"]\n",
    "            if f[\"properties\"][\"ID_es\"] in valor\n",
    "        ]\n",
    "\n",
    "        for i, f in enumerate(features_filtrados):\n",
    "            f[\"id\"] = f.get(\"id\", str(i))\n",
    "\n",
    "    geojson_filtrado = {\n",
    "        \"type\": \"FeatureCollection\",\n",
    "        \"features\": features_filtrados\n",
    "    }\n",
    "\n",
    "    # Crear figura vacía\n",
    "    fig_2 = go.Figure()\n",
    "\n",
    "    # ➤ Capa base: límites del predio (línea negra)\n",
    "    # ➤ Capa base: límites del predio (líneas negras)\n",
    "    for feature in geojson_limite[\"features\"]:\n",
    "        if feature[\"geometry\"][\"type\"] == \"MultiPolygon\":\n",
    "            for polygon in feature[\"geometry\"][\"coordinates\"]:\n",
    "                for ring in polygon:\n",
    "                    lons, lats = zip(*ring)\n",
    "                    fig_2.add_trace(go.Scattermap(\n",
    "                        lon=lons,\n",
    "                        lat=lats,\n",
    "                        mode=\"lines\",\n",
    "                        line=dict(width=2, color=\"black\"),\n",
    "                        name=\"Límite predio\",\n",
    "                        hoverinfo=\"skip\",\n",
    "                        showlegend=False\n",
    "                    ))\n",
    "        elif feature[\"geometry\"][\"type\"] == \"Polygon\":\n",
    "            for ring in feature[\"geometry\"][\"coordinates\"]:\n",
    "                lons, lats = zip(*ring)\n",
    "                fig_2.add_trace(go.Scattermap(\n",
    "                    lon=lons,\n",
    "                    lat=lats,\n",
    "                    mode=\"lines\",\n",
    "                    line=dict(width=2, color=\"black\"),\n",
    "                    name=\"Límite predio\",\n",
    "                    hoverinfo=\"skip\",\n",
    "                    showlegend=False\n",
    "                ))\n",
    "\n",
    "    # ➤ Capa de especies: usando choropleth\n",
    "    fig_map = px.choropleth_map(\n",
    "        geojson=geojson_filtrado,\n",
    "        locations=[f[\"id\"] for f in features_filtrados],\n",
    "        color=[f[\"properties\"][\"Clasificación\"] for f in features_filtrados],\n",
    "        hover_name=[f[\"properties\"][\"Nombre científico\"] for f in features_filtrados],\n",
    "        featureidkey=\"id\",\n",
    "        zoom=13,\n",
    "        center={\"lat\": -39.74, \"lon\": -73.14},\n",
    "        #opacity=0.5\n",
    "    )\n",
    "\n",
    "    # ➤ Añadir la capa de especies sobre la figura existente\n",
    "    for trace in fig_map.data:\n",
    "        fig_2.add_trace(trace)\n",
    "\n",
    "    # Ajustar el diseño final\n",
    "    fig_2.update_layout(\n",
    "        margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0},\n",
    "        map=dict(\n",
    "            style=\"open-street-map\",\n",
    "            zoom=13,\n",
    "            center={\"lat\": -39.74, \"lon\": -73.14}\n",
    "        )\n",
    "    )        \n",
    "        \n",
    "    return fig_predios,fig_predios_lplp,data,columns,encoded_image,fig_pap_año,fig_mapa,fig,fig_2\n",
    "\n",
    "#segundo callback\n",
    "\n",
    "@callback(\n",
    "    Output('tabla','data'),\n",
    "    Output('tabla','columns'),\n",
    "    Input('grafico','clickData')\n",
    "    )\n",
    "\n",
    "def display_click_data(clickData):\n",
    "    if clickData is None:  # Si no hay selección\n",
    "        raise PreventUpdate\n",
    "    else:\n",
    "    \n",
    "       #id=[p['custDataomdata'][0] for p in selectedData['points']]\n",
    "       id=clickData['points'][0]['customdata'][0] #lista\n",
    "       r=[]\n",
    "       for item in pubmed_data:\n",
    "           if item['PMID']in id:\n",
    "               r.append(item)\n",
    "       #aquí tengo que generar el df, no puedo trabajar con isin pq tengo json\n",
    "       pmid=[]\n",
    "       titulo=[]\n",
    "       urla=[]\n",
    "       revista=[]\n",
    "       fecha=[]\n",
    "       autor=[]\n",
    "       inst=[]\n",
    "       for paper in r:\n",
    "           pmid.append(paper['PMID'])\n",
    "           titulo.append(paper['Title']) #aquí se edita el formato del título? json.dumps?\n",
    "           urla.append(paper['URL'])\n",
    "           revista.append(paper['Journal'])\n",
    "           fecha.append(paper['Date']['Month']+\"/\"+paper['Date']['Year'])\n",
    "           autor.append(paper['Authors'][0]['Name']+\" \"+paper['Authors'][0]['Lastname'])\n",
    "           inst.append(\"\".join(paper['Authors'][0]['Affiliations']))\n",
    "\n",
    "           df=pd.DataFrame({'PMID':pmid,'Título':titulo,'1ª autor':autor,'Institución':inst,'URL':urla,'Revista':revista,'Año':fecha})#creo que no es necesario? tal vez para hacer una tabla\n",
    "\n",
    "           data=df.to_dict('records')\n",
    "           columns=[{'id':c,'name':c}for c in df.columns]\n",
    "\n",
    "                  \n",
    "    return data,columns\n",
    "    \n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    app.run_server(port=8053,use_reloader=False,debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
